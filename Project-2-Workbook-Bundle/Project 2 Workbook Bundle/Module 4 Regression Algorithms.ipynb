{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flaws of linear regression\n",
    "- overfitting with many input features\n",
    "- difficult to express non-linear relationships\n",
    "\n",
    "# Algorithms:\n",
    "- Lasso Regression\n",
    "- Ridge Regression\n",
    "- Elastic-net\n",
    "- Random forest\n",
    "- Boosted trees\n",
    "\n",
    "## Overfitting:\n",
    "Overfitting means the machine is memorizing the training data instead of learning the pattern from data.\n",
    "\n",
    "### How overfitting occurs:\n",
    "- model is too complex\n",
    "\n",
    "### Skills to fight overfitting:\n",
    "- regularizations\n",
    "- ensembling\n",
    "- train/test splitting\n",
    "- cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizations\n",
    "\n",
    "## Cost functions\n",
    "- measure the error of predictions\n",
    "- quantify how inaccurate\n",
    "- compare performance accross models\n",
    "- aka loss functions / error functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "- relies on absolute size\n",
    "- coefficient 0 (auto feature selection)\n",
    "- strength of penalty should be tuned\n",
    "- stronger penalty, more coefficient push to 0\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "```\n",
    "\n",
    "# Ridge\n",
    "- relies on squared error\n",
    "- leads to smaller coefficient, doesn't force to 0 (feature shrinkage)\n",
    "- penalty strength should be tuned\n",
    "- stronger penalty, more coefficient push to 0\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "```\n",
    "\n",
    "# Elastic-net\n",
    "- compromise between Lasso and Ridge\n",
    "- combines both L1(absolute) and L2(square) penalties\n",
    "- ratio of L1 and L2 should be tuned\n",
    "- strength of penalty should be tuned\n",
    "- setting ratio to 0 or 1\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import ElasticNet\n",
    "```\n",
    "\n",
    "## Notes:\n",
    "- in some cases, lasso and ridge are easier to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overcome difficulty in expressing non-linear relationships\n",
    "\n",
    "## Ensembling\n",
    "- combining multiple individual models into a single model\n",
    "\n",
    "## Ensembling methods\n",
    "- Bagging\n",
    "    * reduce the chance of overfitting complex models\n",
    "    * train large number of 'strong' learners in **parallel**\n",
    "        - (strong learners is model that's allowed to have high complexity)\n",
    "    * combine all strong learners to smooth out their predictions\n",
    "- Boosting\n",
    "    * improve predictive flexibility of simple models\n",
    "    * train large number of 'weak' learners in **sequence**\n",
    "        - a weak learner is model that has limited complexity\n",
    "    * each one in the sequence focuses on learning from **mistakes** of the one before it\n",
    "    * combines all weak learners into single strong learner\n",
    "    \n",
    "- Both of the methods aimed to approach problems in opposite directions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree ensembling algorithms\n",
    "## Random forests\n",
    "- train large number of **strong decision trees**\n",
    "- combine predictions through bagging\n",
    "- Sources of **randomness**:\n",
    "    * each decision tree only allowed to choose from **random subset of features** to split on\n",
    "    * each decision tree only trained on **random subset of observations** (resampling)\n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "```\n",
    "\n",
    "## Boosted trees\n",
    "- sequeunce of weak, constrained decision trees and combine their predictions through boosting\n",
    "- each decision tree allows a **maximum depth** (should be tuned)\n",
    "- each decision tree tries to correct the prediction errors of the one before it\n",
    "- highest performance ceiling\n",
    "- more complicated than random forests\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
